{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbudyWu/Baseball_Label/blob/main/run_training_dpo_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "kahm-EkpzYej"
      },
      "source": [
        "# Training Pipeline\n",
        "[run_training_dpo_pipeline.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb)    | [Open In Colab](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "JnEi12gPzYel"
      },
      "source": [
        "# Stage 1: Continue Pretraining\n",
        "\n",
        "第一阶段：PT(Continue PreTraining)增量预训练，在海量领域文本数据上二次预训练GPT模型，以适配领域数据分布\n",
        "\n",
        "注意：\n",
        "1. 此阶段是可选的，如果你没有海量领域文本，可以跳过此阶段，直接进行SFT阶段的有监督微调\n",
        "2. 我实验发现：做领域知识注入，SFT比PT更高效，也可以跳过PT阶段\n",
        "\n",
        "| Stage 1: Continue Pretraining   |  [pretraining.py](https://github.com/shibing624/MedicalGPT/blob/main/pretraining.py) | [run_pt.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_pt.sh)    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FxfSb4szYel"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是Qwen/Qwen2.5-0.5B\n",
        "2. 数据集：PT阶段使用的是中文天龙八部小说部分文本和英文书籍部分文本，位于`data/pretrain`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "SvrQ2X86zYem"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UI6odiazYem"
      },
      "source": [
        "## 配置运行环境\n",
        "\n",
        "本地执行可注释以下配置环境的命令，colab执行要打开注释，用于配置环境\n",
        "\n",
        "colab建议使用T4 GPU训练，设置方式：`代码执行程序 -> 更改运行时类型 -> 运行时类型：Python3，硬件加速器：GPU，GPU类型：T4 -> 保存`\n",
        "\n",
        "步骤：\n",
        "1. 下载最新代码到本地\n",
        "2. 安装依赖包\n",
        "\n",
        "依赖包如下，保证最新版本：\n",
        "\n",
        "```\n",
        "loguru\n",
        "transformers\n",
        "sentencepiece\n",
        "datasets\n",
        "tensorboard\n",
        "tqdm\n",
        "peft\n",
        "trl\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xxi_PeLGzYem",
        "outputId": "43273565-c850-44b9-c5ae-23617f409df8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'MedicalGPT' already exists and is not an empty directory.\n",
            "/content/MedicalGPT\n",
            "build_domain_tokenizer.py   README.md\n",
            "chatpdf.py                  requirements.txt\n",
            "CITATION.cff                reward_modeling.py\n",
            "_config.yml                 \u001b[0m\u001b[01;34mrole_play_data\u001b[0m/\n",
            "CONTRIBUTING.md             run_dpo.sh\n",
            "convert_dataset.py          run_eval_quantize.sh\n",
            "\u001b[01;34mdata\u001b[0m/                       run_full_sft.sh\n",
            "DISCLAIMER                  run_grpo.sh\n",
            "\u001b[01;34mdocs\u001b[0m/                       run_orpo.sh\n",
            "dpo_training.py             run_ppo.sh\n",
            "eval_quantize.py            run_pt.sh\n",
            "fastapi_server_demo.py      run_quant.sh\n",
            "gradio_demo.py              run_rm.sh\n",
            "grpo_training.py            run_sft.sh\n",
            "inference_multigpu_demo.py  run_training_dpo_pipeline.ipynb\n",
            "inference.py                run_training_ppo_pipeline.ipynb\n",
            "LICENSE                     supervised_finetuning.py\n",
            "\u001b[01;34mMedicalGPT\u001b[0m/                 template.py\n",
            "merge_peft_adapter.py       validate_jsonl.py\n",
            "merge_tokenizers.py         vllm_deployment.sh\n",
            "model_quant.py              zero1.yaml\n",
            "openai_api.py               zero2.json\n",
            "orpo_training.py            zero2.yaml\n",
            "ppo_training.py             zero3.json\n",
            "pretraining.py              zero3.yaml\n",
            "README_EN.md\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (1.5.2)\n",
            "Requirement already satisfied: datasets>=2.14.6 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (3.5.0)\n",
            "Requirement already satisfied: loguru in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (0.7.3)\n",
            "Collecting peft>=0.14.0 (from -r requirements.txt (line 4))\n",
            "  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (1.6.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (2.18.0)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (4.67.1)\n",
            "Collecting transformers>=4.49.0 (from -r requirements.txt (line 9))\n",
            "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting trl==0.15.2 (from -r requirements.txt (line 10))\n",
            "  Using cached trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: latex2sympy2_extended in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (1.0.6)\n",
            "Requirement already satisfied: math-verify==0.5.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (0.5.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl==0.15.2->-r requirements.txt (line 10)) (13.9.4)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.13.2 in /usr/local/lib/python3.11/dist-packages (from latex2sympy2_extended->-r requirements.txt (line 12)) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from latex2sympy2_extended->-r requirements.txt (line 12)) (1.13.1)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate->-r requirements.txt (line 1)) (2.2.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate->-r requirements.txt (line 1)) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate->-r requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate->-r requirements.txt (line 1)) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate->-r requirements.txt (line 1)) (0.30.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate->-r requirements.txt (line 1)) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (3.11.15)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.8)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 7)) (5.29.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 7)) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 7)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.1.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.49.0->-r requirements.txt (line 9)) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.49.0->-r requirements.txt (line 9)) (0.21.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2)) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2)) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2)) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2)) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate->-r requirements.txt (line 1)) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->latex2sympy2_extended->-r requirements.txt (line 12)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 7)) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl==0.15.2->-r requirements.txt (line 10)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl==0.15.2->-r requirements.txt (line 10)) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl==0.15.2->-r requirements.txt (line 10)) (0.1.2)\n",
            "Using cached trl-0.15.2-py3-none-any.whl (318 kB)\n",
            "Downloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
            "Installing collected packages: transformers, trl, peft\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.48.2\n",
            "    Uninstalling transformers-4.48.2:\n",
            "      Successfully uninstalled transformers-4.48.2\n",
            "  Attempting uninstall: trl\n",
            "    Found existing installation: trl 0.8.6\n",
            "    Uninstalling trl-0.8.6:\n",
            "      Successfully uninstalled trl-0.8.6\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.11.0\n",
            "    Uninstalling peft-0.11.0:\n",
            "      Successfully uninstalled peft-0.11.0\n",
            "Successfully installed peft-0.15.2 transformers-4.51.3 trl-0.15.2\n",
            "Collecting transformers==4.48.2\n",
            "  Using cached transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
            "Collecting filelock (from transformers==4.48.2)\n",
            "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers==4.48.2)\n",
            "  Using cached huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting numpy>=1.17 (from transformers==4.48.2)\n",
            "  Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting packaging>=20.0 (from transformers==4.48.2)\n",
            "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting pyyaml>=5.1 (from transformers==4.48.2)\n",
            "  Using cached PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers==4.48.2)\n",
            "  Using cached regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting requests (from transformers==4.48.2)\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers==4.48.2)\n",
            "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting safetensors>=0.4.1 (from transformers==4.48.2)\n",
            "  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting tqdm>=4.27 (from transformers==4.48.2)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.2)\n",
            "  Using cached fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.2)\n",
            "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests->transformers==4.48.2)\n",
            "  Using cached charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->transformers==4.48.2)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->transformers==4.48.2)\n",
            "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->transformers==4.48.2)\n",
            "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
            "Using cached transformers-4.48.2-py3-none-any.whl (9.7 MB)\n",
            "Using cached huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
            "Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
            "Using cached PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
            "Using cached regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
            "Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
            "Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
            "Using cached charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
            "Using cached fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
            "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "Installing collected packages: urllib3, typing-extensions, tqdm, safetensors, regex, pyyaml, packaging, numpy, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.4.0\n",
            "    Uninstalling urllib3-2.4.0:\n",
            "      Successfully uninstalled urllib3-2.4.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.13.2\n",
            "    Uninstalling typing_extensions-4.13.2:\n",
            "      Successfully uninstalled typing_extensions-4.13.2\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: safetensors\n",
            "    Found existing installation: safetensors 0.5.3\n",
            "    Uninstalling safetensors-0.5.3:\n",
            "      Successfully uninstalled safetensors-0.5.3\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.11.6\n",
            "    Uninstalling regex-2024.11.6:\n",
            "      Successfully uninstalled regex-2024.11.6\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.2\n",
            "    Uninstalling PyYAML-6.0.2:\n",
            "      Successfully uninstalled PyYAML-6.0.2\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.5\n",
            "    Uninstalling numpy-2.2.5:\n",
            "      Successfully uninstalled numpy-2.2.5\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.12.0\n",
            "    Uninstalling fsspec-2024.12.0:\n",
            "      Successfully uninstalled fsspec-2024.12.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.18.0\n",
            "    Uninstalling filelock-3.18.0:\n",
            "      Successfully uninstalled filelock-3.18.0\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.1\n",
            "    Uninstalling charset-normalizer-3.4.1:\n",
            "      Successfully uninstalled charset-normalizer-3.4.1\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.1.31\n",
            "    Uninstalling certifi-2025.1.31:\n",
            "      Successfully uninstalled certifi-2025.1.31\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.30.2\n",
            "    Uninstalling huggingface-hub-0.30.2:\n",
            "      Successfully uninstalled huggingface-hub-0.30.2\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 3.5.0 requires fsspec[http]<=2024.12.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\n",
            "langchain-core 0.3.55 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed certifi-2025.1.31 charset-normalizer-3.4.1 filelock-3.18.0 fsspec-2025.3.2 huggingface-hub-0.30.2 idna-3.10 numpy-2.2.5 packaging-25.0 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.3 tokenizers-0.21.1 tqdm-4.67.1 transformers-4.48.2 typing-extensions-4.13.2 urllib3-2.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi"
                ]
              },
              "id": "19e0e07591cb4c41b9ba5a6079d881c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting peft==0.11.0\n",
            "  Using cached peft-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting trl==0.8.6\n",
            "  Using cached trl-0.8.6-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft==0.11.0) (2.2.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.11.0) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.11.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft==0.11.0) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.11.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft==0.11.0) (4.48.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft==0.11.0) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.11.0) (1.5.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft==0.11.0) (0.5.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.11.0) (0.30.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from trl==0.8.6) (3.5.0)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.11/dist-packages (from trl==0.8.6) (0.9.19)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.11.0) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.11.0) (2025.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.11.0) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.11.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.11.0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft==0.11.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft==0.11.0) (0.21.1)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.8.6) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.8.6) (13.9.4)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.8.6) (1.7.2)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.8.6) (4.4.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.8.6) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.8.6) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.8.6) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.8.6) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.8.6) (0.70.16)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.17.0->peft==0.11.0)\n",
            "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.8.6) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.8.6) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.8.6) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.8.6) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.8.6) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.8.6) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.8.6) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.8.6) (1.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.0) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (2.18.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft==0.11.0) (3.0.2)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "^C\n",
            "4.48.2\n"
          ]
        }
      ],
      "source": [
        "!git clone --depth 1 https://github.com/shibing624/MedicalGPT.git\n",
        "%cd MedicalGPT\n",
        "%ls\n",
        "\n",
        "# 安裝 requirements.txt 中的依賴（允許子依賴）\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# 強制安裝 transformers 4.48.2，覆蓋 requirements.txt 中的版本\n",
        "!pip install transformers==4.48.2 --force-reinstall\n",
        "\n",
        "# 安裝相容版本的 peft 和 trl\n",
        "!pip install peft==0.11.0 trl==0.8.6\n",
        "\n",
        "# 驗證 transformers 版本\n",
        "import transformers\n",
        "print(transformers.__version__)  # 應輸出 4.48.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RbJ84hTzYen"
      },
      "source": [
        "## Stage1 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载模型和tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果\n",
        "\n",
        "**以下参数可以根据你的GPU实际情况修改，当前参数是根据Colab的T4单卡GPU（16GB显存）配置的**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5jDAnT5QzYen",
        "outputId": "0d0478bb-20d9-415c-df1f-50514bf0cfa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access './data/pretrain/': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "%ls ./data/pretrain/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls ./MedicalGPT/"
      ],
      "metadata": {
        "id": "4Qh9ZH8h3fvD",
        "outputId": "a852176e-d5a9-452a-a565-b3e479ee7b0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build_domain_tokenizer.py   README.md\n",
            "chatpdf.py                  requirements.txt\n",
            "CITATION.cff                reward_modeling.py\n",
            "_config.yml                 \u001b[0m\u001b[01;34mrole_play_data\u001b[0m/\n",
            "CONTRIBUTING.md             run_dpo.sh\n",
            "convert_dataset.py          run_eval_quantize.sh\n",
            "\u001b[01;34mdata\u001b[0m/                       run_full_sft.sh\n",
            "DISCLAIMER                  run_grpo.sh\n",
            "\u001b[01;34mdocs\u001b[0m/                       run_orpo.sh\n",
            "dpo_training.py             run_ppo.sh\n",
            "eval_quantize.py            run_pt.sh\n",
            "fastapi_server_demo.py      run_quant.sh\n",
            "gradio_demo.py              run_rm.sh\n",
            "grpo_training.py            run_sft.sh\n",
            "inference_multigpu_demo.py  run_training_dpo_pipeline.ipynb\n",
            "inference.py                run_training_ppo_pipeline.ipynb\n",
            "LICENSE                     supervised_finetuning.py\n",
            "\u001b[01;34mMedicalGPT\u001b[0m/                 template.py\n",
            "merge_peft_adapter.py       validate_jsonl.py\n",
            "merge_tokenizers.py         vllm_deployment.sh\n",
            "model_quant.py              zero1.yaml\n",
            "openai_api.py               zero2.json\n",
            "orpo_training.py            zero2.yaml\n",
            "ppo_training.py             zero3.json\n",
            "pretraining.py              zero3.yaml\n",
            "README_EN.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mOoBirsDzYen",
        "outputId": "64e9bd2e-6c00-4a8b-a4f1-1e78a9a82908",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-25 16:58:31.398799: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745600311.419120    7150 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745600311.425602    7150 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-25 16:58:31.446972: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "\u001b[32m2025-04-25 16:58:34.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mModel args: ModelArguments(model_name_or_path='Qwen/Qwen2.5-0.5B', tokenizer_name_or_path=None, load_in_8bit=False, load_in_4bit=False, cache_dir=None, model_revision='main', hf_hub_token=None, use_fast_tokenizer=False, torch_dtype='bfloat16', device_map='auto', trust_remote_code=True)\u001b[0m\n",
            "\u001b[32m2025-04-25 16:58:34.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mData args: DataArguments(dataset_name=None, dataset_config_name=None, train_file_dir='./MedicalGPT/data/pretrain', validation_file_dir='./MedicalGPT/data/pretrain', max_train_samples=20000, max_eval_samples=10, streaming=False, block_size=128, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=1, keep_linebreaks=True)\u001b[0m\n",
            "\u001b[32m2025-04-25 16:58:34.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m361\u001b[0m - \u001b[1mTraining args: Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=True,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=30000,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=50,\n",
            "eval_strategy=IntervalStrategy.STEPS,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=True,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=True,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0002,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=outputs-pt-v1/runs/Apr25_16-58-34_d8f6cbbf2f65,\n",
            "logging_first_step=True,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=OptimizerNames.ADAMW_TORCH,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=outputs-pt-v1,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=3,\n",
            "per_device_train_batch_size=3,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=outputs-pt-v1,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=50,\n",
            "save_strategy=SaveStrategy.STEPS,\n",
            "save_total_limit=3,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.05,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.01,\n",
            ")\u001b[0m\n",
            "\u001b[32m2025-04-25 16:58:34.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1mScript args: ScriptArguments(use_peft=True, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, qlora=False)\u001b[0m\n",
            "\u001b[32m2025-04-25 16:58:34.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m363\u001b[0m - \u001b[1mProcess rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\u001b[0m\n",
            "\u001b[32m2025-04-25 16:58:35.229\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m471\u001b[0m - \u001b[1mtrain files: ['./MedicalGPT/data/pretrain/tianlongbabu.txt', './MedicalGPT/data/pretrain/en_article_tail500.txt', './MedicalGPT/data/pretrain/fever.txt']\u001b[0m\n",
            "\u001b[32m2025-04-25 16:58:35.230\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m481\u001b[0m - \u001b[1meval files: ['./MedicalGPT/data/pretrain/tianlongbabu.txt', './MedicalGPT/data/pretrain/en_article_tail500.txt', './MedicalGPT/data/pretrain/fever.txt']\u001b[0m\n",
            "Generating train split: 3876 examples [00:00, 109825.39 examples/s]\n",
            "Generating validation split: 3876 examples [00:00, 401727.84 examples/s]\n",
            "\u001b[32m2025-04-25 16:58:36.094\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m513\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 3876\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 3876\n",
            "    })\n",
            "})\u001b[0m\n",
            "Running tokenizer on dataset: 100% 3876/3876 [00:10<00:00, 360.18 examples/s]\n",
            "Running tokenizer on dataset: 100% 3876/3876 [00:10<00:00, 364.60 examples/s]\n",
            "Grouping texts in chunks of 128: 100% 3876/3876 [00:00<00:00, 9298.48 examples/s]\n",
            "Grouping texts in chunks of 128: 100% 3876/3876 [00:00<00:00, 9550.80 examples/s] \n",
            "\u001b[32m2025-04-25 16:59:03.331\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m576\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 2502\u001b[0m\n",
            "\u001b[32m2025-04-25 16:59:03.332\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m577\u001b[0m - \u001b[34m\u001b[1mTokenized training example:\u001b[0m\n",
            "\u001b[32m2025-04-25 16:59:03.334\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m578\u001b[0m - \u001b[34m\u001b[1m天龙八部\n",
            "\n",
            "\n",
            "正文 释名\n",
            "“天龙八部”这名词出于佛经。许多大乘佛经叙述佛向诸菩萨、比丘等说法时，崐常有天龙八部参与听法。如“法华经：提婆达多品”：“天龙八部、人与非人，皆崐遥见彼龙女成佛”。\n",
            "“非人”，包括八种神道怪物，因为以“天”及“龙”为首，崐所以称为《天龙八部》。八部罗，七归那\u001b[0m\n",
            "\u001b[32m2025-04-25 16:59:03.336\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m590\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
            "\u001b[32m2025-04-25 16:59:03.336\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m591\u001b[0m - \u001b[34m\u001b[1mTokenized eval example:\u001b[0m\n",
            "\u001b[32m2025-04-25 16:59:03.337\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m592\u001b[0m - \u001b[34m\u001b[1m天龙八部\n",
            "\n",
            "\n",
            "正文 释名\n",
            "“天龙八部”这名词出于佛经。许多大乘佛经叙述佛向诸菩萨、比丘等说法时，崐常有天龙八部参与听法。如“法华经：提婆达多品”：“天龙八部、人与非人，皆崐遥见彼龙女成佛”。\n",
            "“非人”，包括八种神道怪物，因为以“天”及“龙”为首，崐所以称为《天龙八部》。八部罗，七归那\u001b[0m\n",
            "config.json: 100% 681/681 [00:00<00:00, 4.29MB/s]\n",
            "model.safetensors: 100% 988M/988M [00:03<00:00, 248MB/s]\n",
            "generation_config.json: 100% 138/138 [00:00<00:00, 913kB/s]\n",
            "\u001b[32m2025-04-25 16:59:09.613\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m651\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
            "\u001b[32m2025-04-25 16:59:09.614\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m656\u001b[0m - \u001b[1mInit new peft model\u001b[0m\n",
            "\u001b[32m2025-04-25 16:59:09.614\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m669\u001b[0m - \u001b[1mPeft target_modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[0m\n",
            "\u001b[32m2025-04-25 16:59:09.614\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m670\u001b[0m - \u001b[1mPeft lora_rank: 8\u001b[0m\n",
            "trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py:644: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.\n",
            "  warnings.warn(\n",
            "/content/./MedicalGPT/pretraining.py:700: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SavePeftModelTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = SavePeftModelTrainer(\n",
            "\u001b[32m2025-04-25 16:59:10.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m715\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
            "\u001b[32m2025-04-25 16:59:10.620\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m716\u001b[0m - \u001b[34m\u001b[1mTrain dataloader example: {'input_ids': tensor([[ 64272,   3837,  99364,  91676, 100292,  99296,   8997,  99212,  57750,\n",
            "         104462,  49567,  34187, 111502,   3837,  88970,  42411, 102080,  19403,\n",
            "          70074,   3837, 104094,  36987, 107314,   3837,  41406,  56568, 113240,\n",
            "         104543,  75758,    198,  37474,  99783,  44793,  36987,  35946,  49187,\n",
            "         105665,  49828, 108145,   3837, 114811,  63109, 104543,   3837, 104984,\n",
            "          97706,  99756,  34187,  56568, 106669,  81264,    198,  99212,  57750,\n",
            "         104462, 104494,  99592, 109760,   3837, 101611,  99313,  99234,  22243,\n",
            "           3837,  56652, 106718, 109482,  63703, 110861,   3837,  99508, 100524,\n",
            "          99793,   1773, 100147,  36407,  37474,  99783,  30440,  80158,  99746,\n",
            "          34187,   3837,  64355,  99684,  44934,  99336,  89012,  44793, 101913,\n",
            "          82647,  99385, 101432,  49828, 112656, 101724, 119463,   1773,  99212,\n",
            "          57750, 104462,  99882,  44793,  36987,  56568,  79072,  16530, 113206,\n",
            "          81264,  37474,  99783, 108585, 104543,  44793,  36987,  56568,  43288,\n",
            "         109593,  52801, 119026,   9370, 103357, 100897, 104340,  75758,  99212,\n",
            "          57750, 104462],\n",
            "        [  3837, 115041, 100090,  99805,   3837,  69041,  63703, 105967, 108739,\n",
            "         111897,  64355,   1773,  63703, 105967, 105626, 101760,   1773, 113251,\n",
            "         100432,  44793,  36987,  99783,  99261,   3837,  56568,  99665, 103335,\n",
            "         110460,   3837, 104219,  99245,  74763,  62922,  99172,   3837, 105418,\n",
            "          33126, 100186,  32555,  99369,  17177, 109913,   3837, 102786,  99418,\n",
            "         100406,  99309, 108681,   3837, 104201,  20412,  50511,  18830,  53930,\n",
            "          46423,   3837, 105622,  99851, 102633,  32945,  37474,  99783, 106101,\n",
            "          34187,   3837,  99539,  77144,  99901,  22382,   8997,  21894,  99237,\n",
            "          33108, 100288, 107899,  71618, 108928, 113916,   3837,  48934,  14777,\n",
            "         100898,  99180,   3837,  99364,  59879,  18493,  37474,  99783,  33447,\n",
            "          99931,   9370,  99208,  99468, 102811,  17447,   3837,  14777,  82075,\n",
            "          63367,  47534, 116345,  99844,  17254,   1773,  99212,  99208,  99468,\n",
            "         102811,  99372,  28291,  99326,  14777, 101490,   3837, 100409,  99625,\n",
            "         100297,   1773, 104578,  21894,  48921, 109503,  27442,  42411,  48888,\n",
            "         100297, 101168],\n",
            "        [ 56006, 102346,   1773,  56568, 107575, 111285, 107558,  56006, 103954,\n",
            "           3837,  35946,  14777,  36407,  99670,   3837,  40820,  36407,  99756,\n",
            "          25710,   3837,  44991,  36407,  99756, 100406,   3837,  63703,  36407,\n",
            "          99756,  99561,   3837, 101886,  20412,  16530,  56006,   9370,   1773,\n",
            "         101435,  16530,  56006,   3837,  99486,  16530,  56006,  32945,    198,\n",
            "          42411,  43288,  86117, 107585,   2073,  56568, 107575,    854,   2073,\n",
            "          35946, 107575,  97907,   3837, 109622, 108228, 119326,  39426,  99738,\n",
            "         100141,   3837,  99930,  99669,  99928,  15946, 108264, 110311,   3837,\n",
            "         106118, 103206,  99898,  53647,  42192,  32757, 103954,    854,  60686,\n",
            "         100401,  99493,  79766,  64689,  16872, 105569,  99200,  99571,  41146,\n",
            "          99369,   3837,  52801, 116446,  57750, 105770,  33983,  33983, 105219,\n",
            "          48738,   1773,  99930,  99669,  99928,  17447,  99939, 106625, 105696,\n",
            "           9370, 105632,   3837, 119501,  20450,  14777,  99864,  42192,  99712,\n",
            "           8997, 111315,  99225, 102028,  26288, 100875,  64682, 101180,   3837,\n",
            "         100867, 103954]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'labels': tensor([[ 64272,   3837,  99364,  91676, 100292,  99296,   8997,  99212,  57750,\n",
            "         104462,  49567,  34187, 111502,   3837,  88970,  42411, 102080,  19403,\n",
            "          70074,   3837, 104094,  36987, 107314,   3837,  41406,  56568, 113240,\n",
            "         104543,  75758,    198,  37474,  99783,  44793,  36987,  35946,  49187,\n",
            "         105665,  49828, 108145,   3837, 114811,  63109, 104543,   3837, 104984,\n",
            "          97706,  99756,  34187,  56568, 106669,  81264,    198,  99212,  57750,\n",
            "         104462, 104494,  99592, 109760,   3837, 101611,  99313,  99234,  22243,\n",
            "           3837,  56652, 106718, 109482,  63703, 110861,   3837,  99508, 100524,\n",
            "          99793,   1773, 100147,  36407,  37474,  99783,  30440,  80158,  99746,\n",
            "          34187,   3837,  64355,  99684,  44934,  99336,  89012,  44793, 101913,\n",
            "          82647,  99385, 101432,  49828, 112656, 101724, 119463,   1773,  99212,\n",
            "          57750, 104462,  99882,  44793,  36987,  56568,  79072,  16530, 113206,\n",
            "          81264,  37474,  99783, 108585, 104543,  44793,  36987,  56568,  43288,\n",
            "         109593,  52801, 119026,   9370, 103357, 100897, 104340,  75758,  99212,\n",
            "          57750, 104462],\n",
            "        [  3837, 115041, 100090,  99805,   3837,  69041,  63703, 105967, 108739,\n",
            "         111897,  64355,   1773,  63703, 105967, 105626, 101760,   1773, 113251,\n",
            "         100432,  44793,  36987,  99783,  99261,   3837,  56568,  99665, 103335,\n",
            "         110460,   3837, 104219,  99245,  74763,  62922,  99172,   3837, 105418,\n",
            "          33126, 100186,  32555,  99369,  17177, 109913,   3837, 102786,  99418,\n",
            "         100406,  99309, 108681,   3837, 104201,  20412,  50511,  18830,  53930,\n",
            "          46423,   3837, 105622,  99851, 102633,  32945,  37474,  99783, 106101,\n",
            "          34187,   3837,  99539,  77144,  99901,  22382,   8997,  21894,  99237,\n",
            "          33108, 100288, 107899,  71618, 108928, 113916,   3837,  48934,  14777,\n",
            "         100898,  99180,   3837,  99364,  59879,  18493,  37474,  99783,  33447,\n",
            "          99931,   9370,  99208,  99468, 102811,  17447,   3837,  14777,  82075,\n",
            "          63367,  47534, 116345,  99844,  17254,   1773,  99212,  99208,  99468,\n",
            "         102811,  99372,  28291,  99326,  14777, 101490,   3837, 100409,  99625,\n",
            "         100297,   1773, 104578,  21894,  48921, 109503,  27442,  42411,  48888,\n",
            "         100297, 101168],\n",
            "        [ 56006, 102346,   1773,  56568, 107575, 111285, 107558,  56006, 103954,\n",
            "           3837,  35946,  14777,  36407,  99670,   3837,  40820,  36407,  99756,\n",
            "          25710,   3837,  44991,  36407,  99756, 100406,   3837,  63703,  36407,\n",
            "          99756,  99561,   3837, 101886,  20412,  16530,  56006,   9370,   1773,\n",
            "         101435,  16530,  56006,   3837,  99486,  16530,  56006,  32945,    198,\n",
            "          42411,  43288,  86117, 107585,   2073,  56568, 107575,    854,   2073,\n",
            "          35946, 107575,  97907,   3837, 109622, 108228, 119326,  39426,  99738,\n",
            "         100141,   3837,  99930,  99669,  99928,  15946, 108264, 110311,   3837,\n",
            "         106118, 103206,  99898,  53647,  42192,  32757, 103954,    854,  60686,\n",
            "         100401,  99493,  79766,  64689,  16872, 105569,  99200,  99571,  41146,\n",
            "          99369,   3837,  52801, 116446,  57750, 105770,  33983,  33983, 105219,\n",
            "          48738,   1773,  99930,  99669,  99928,  17447,  99939, 106625, 105696,\n",
            "           9370, 105632,   3837, 119501,  20450,  14777,  99864,  42192,  99712,\n",
            "           8997, 111315,  99225, 102028,  26288, 100875,  64682, 101180,   3837,\n",
            "         100867, 103954]], device='cuda:0')}\u001b[0m\n",
            "{'loss': 3.9486, 'grad_norm': 2.5674479007720947, 'learning_rate': 4.7619047619047615e-06, 'epoch': 0.0}\n",
            "{'loss': 3.8622, 'grad_norm': 3.185664176940918, 'learning_rate': 4.761904761904762e-05, 'epoch': 0.01}\n",
            "{'loss': 3.8422, 'grad_norm': 2.1380414962768555, 'learning_rate': 9.523809523809524e-05, 'epoch': 0.02}\n",
            "{'loss': 3.7712, 'grad_norm': 2.5635485649108887, 'learning_rate': 0.00014285714285714287, 'epoch': 0.04}\n",
            "{'loss': 3.6083, 'grad_norm': 3.418485641479492, 'learning_rate': 0.00019047619047619048, 'epoch': 0.05}\n",
            "{'loss': 3.5338, 'grad_norm': 2.673893928527832, 'learning_rate': 0.000197979797979798, 'epoch': 0.06}\n",
            "  6% 50/834 [00:32<08:18,  1.57it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 10.28it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 4.2440080642700195, 'eval_accuracy': 0.3165354330708661, 'eval_runtime': 0.7649, 'eval_samples_per_second': 13.073, 'eval_steps_per_second': 5.229, 'epoch': 0.06}\n",
            "  6% 50/834 [00:33<08:18,  1.57it/s]\n",
            "100% 4/4 [00:00<00:00,  7.24it/s]\u001b[A\n",
            "{'loss': 3.668, 'grad_norm': 2.5929951667785645, 'learning_rate': 0.00019545454545454548, 'epoch': 0.07}\n",
            "{'loss': 3.5672, 'grad_norm': 2.592829704284668, 'learning_rate': 0.00019292929292929293, 'epoch': 0.08}\n",
            "{'loss': 3.5946, 'grad_norm': 2.526561737060547, 'learning_rate': 0.0001904040404040404, 'epoch': 0.1}\n",
            "{'loss': 3.4535, 'grad_norm': 2.616374969482422, 'learning_rate': 0.0001878787878787879, 'epoch': 0.11}\n",
            "{'loss': 3.5857, 'grad_norm': 2.214794635772705, 'learning_rate': 0.00018535353535353537, 'epoch': 0.12}\n",
            " 12% 100/834 [01:05<07:49,  1.56it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.64it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.78it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.150481700897217, 'eval_accuracy': 0.31496062992125984, 'eval_runtime': 0.733, 'eval_samples_per_second': 13.643, 'eval_steps_per_second': 5.457, 'epoch': 0.12}\n",
            " 12% 100/834 [01:06<07:49,  1.56it/s]\n",
            "100% 4/4 [00:00<00:00,  7.38it/s]\u001b[A\n",
            "{'loss': 3.4103, 'grad_norm': 2.8251562118530273, 'learning_rate': 0.00018282828282828283, 'epoch': 0.13}\n",
            "{'loss': 3.6384, 'grad_norm': 2.559725046157837, 'learning_rate': 0.0001803030303030303, 'epoch': 0.14}\n",
            "{'loss': 3.7222, 'grad_norm': 2.587604284286499, 'learning_rate': 0.00017777777777777779, 'epoch': 0.16}\n",
            "{'loss': 3.4926, 'grad_norm': 2.664011001586914, 'learning_rate': 0.00017525252525252527, 'epoch': 0.17}\n",
            "{'loss': 3.4099, 'grad_norm': 2.4957058429718018, 'learning_rate': 0.00017272727272727275, 'epoch': 0.18}\n",
            " 18% 150/834 [01:39<07:29,  1.52it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.51it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.63it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.265793800354004, 'eval_accuracy': 0.3094488188976378, 'eval_runtime': 0.7504, 'eval_samples_per_second': 13.327, 'eval_steps_per_second': 5.331, 'epoch': 0.18}\n",
            " 18% 150/834 [01:40<07:29,  1.52it/s]\n",
            "100% 4/4 [00:00<00:00,  7.21it/s]\u001b[A\n",
            "{'loss': 3.6841, 'grad_norm': 2.5724475383758545, 'learning_rate': 0.0001702020202020202, 'epoch': 0.19}\n",
            "{'loss': 3.6246, 'grad_norm': 2.7714951038360596, 'learning_rate': 0.00016767676767676768, 'epoch': 0.2}\n",
            "{'loss': 3.5579, 'grad_norm': 2.6263575553894043, 'learning_rate': 0.00016515151515151516, 'epoch': 0.22}\n",
            "{'loss': 3.5975, 'grad_norm': 2.550999879837036, 'learning_rate': 0.00016262626262626264, 'epoch': 0.23}\n",
            "{'loss': 3.4057, 'grad_norm': 2.459834337234497, 'learning_rate': 0.00016010101010101012, 'epoch': 0.24}\n",
            " 24% 200/834 [02:14<07:04,  1.49it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.19it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.51it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.246382236480713, 'eval_accuracy': 0.3173228346456693, 'eval_runtime': 0.7633, 'eval_samples_per_second': 13.101, 'eval_steps_per_second': 5.24, 'epoch': 0.24}\n",
            " 24% 200/834 [02:15<07:04,  1.49it/s]\n",
            "100% 4/4 [00:00<00:00,  7.08it/s]\u001b[A\n",
            "{'loss': 3.5539, 'grad_norm': 2.507934808731079, 'learning_rate': 0.00015757575757575757, 'epoch': 0.25}\n",
            "{'loss': 3.59, 'grad_norm': 2.496752977371216, 'learning_rate': 0.00015505050505050508, 'epoch': 0.26}\n",
            "{'loss': 3.3424, 'grad_norm': 2.7127413749694824, 'learning_rate': 0.00015252525252525253, 'epoch': 0.28}\n",
            "{'loss': 3.4715, 'grad_norm': 5.482004642486572, 'learning_rate': 0.00015000000000000001, 'epoch': 0.29}\n",
            "{'loss': 3.4842, 'grad_norm': 2.43325138092041, 'learning_rate': 0.00014747474747474747, 'epoch': 0.3}\n",
            " 30% 250/834 [02:49<06:38,  1.46it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.65it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.26it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.180901527404785, 'eval_accuracy': 0.315748031496063, 'eval_runtime': 0.7824, 'eval_samples_per_second': 12.782, 'eval_steps_per_second': 5.113, 'epoch': 0.3}\n",
            " 30% 250/834 [02:50<06:38,  1.46it/s]\n",
            "100% 4/4 [00:00<00:00,  6.86it/s]\u001b[A\n",
            "{'loss': 3.2996, 'grad_norm': 2.2291176319122314, 'learning_rate': 0.00014494949494949495, 'epoch': 0.31}\n",
            "{'loss': 3.5083, 'grad_norm': 2.8246982097625732, 'learning_rate': 0.00014242424242424243, 'epoch': 0.32}\n",
            "{'loss': 3.4178, 'grad_norm': 2.393993616104126, 'learning_rate': 0.0001398989898989899, 'epoch': 0.34}\n",
            "{'loss': 3.3125, 'grad_norm': 2.257305145263672, 'learning_rate': 0.0001373737373737374, 'epoch': 0.35}\n",
            "{'loss': 3.4519, 'grad_norm': 2.31265926361084, 'learning_rate': 0.00013484848484848484, 'epoch': 0.36}\n",
            " 36% 300/834 [03:25<06:11,  1.44it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.98it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.35it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.101630210876465, 'eval_accuracy': 0.32598425196850395, 'eval_runtime': 0.7834, 'eval_samples_per_second': 12.766, 'eval_steps_per_second': 5.106, 'epoch': 0.36}\n",
            " 36% 300/834 [03:26<06:11,  1.44it/s]\n",
            "100% 4/4 [00:00<00:00,  6.89it/s]\u001b[A\n",
            "{'loss': 3.412, 'grad_norm': 2.2678444385528564, 'learning_rate': 0.00013232323232323235, 'epoch': 0.37}\n",
            "{'loss': 3.464, 'grad_norm': 2.4331648349761963, 'learning_rate': 0.0001297979797979798, 'epoch': 0.38}\n",
            "{'loss': 3.4093, 'grad_norm': 2.8279645442962646, 'learning_rate': 0.00012727272727272728, 'epoch': 0.4}\n",
            "{'loss': 3.3848, 'grad_norm': 2.4644839763641357, 'learning_rate': 0.00012474747474747473, 'epoch': 0.41}\n",
            "{'loss': 3.6303, 'grad_norm': 2.486109972000122, 'learning_rate': 0.00012222222222222224, 'epoch': 0.42}\n",
            " 42% 350/834 [04:02<05:34,  1.45it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.87it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.23it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.126989841461182, 'eval_accuracy': 0.3228346456692913, 'eval_runtime': 0.7856, 'eval_samples_per_second': 12.73, 'eval_steps_per_second': 5.092, 'epoch': 0.42}\n",
            " 42% 350/834 [04:03<05:34,  1.45it/s]\n",
            "100% 4/4 [00:00<00:00,  6.83it/s]\u001b[A\n",
            "{'loss': 3.4729, 'grad_norm': 2.604285478591919, 'learning_rate': 0.00011969696969696971, 'epoch': 0.43}\n",
            "{'loss': 3.7115, 'grad_norm': 2.9817497730255127, 'learning_rate': 0.00011717171717171717, 'epoch': 0.44}\n",
            "{'loss': 3.293, 'grad_norm': 2.406843423843384, 'learning_rate': 0.00011464646464646464, 'epoch': 0.46}\n",
            "{'loss': 3.4688, 'grad_norm': 2.46543025970459, 'learning_rate': 0.00011212121212121212, 'epoch': 0.47}\n",
            "{'loss': 3.409, 'grad_norm': 2.620109796524048, 'learning_rate': 0.0001095959595959596, 'epoch': 0.48}\n",
            " 48% 400/834 [04:38<04:59,  1.45it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.80it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.36it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.142172336578369, 'eval_accuracy': 0.3283464566929134, 'eval_runtime': 0.782, 'eval_samples_per_second': 12.787, 'eval_steps_per_second': 5.115, 'epoch': 0.48}\n",
            " 48% 400/834 [04:38<04:59,  1.45it/s]\n",
            "100% 4/4 [00:00<00:00,  6.94it/s]\u001b[A\n",
            "{'loss': 3.2516, 'grad_norm': 2.1889705657958984, 'learning_rate': 0.00010707070707070708, 'epoch': 0.49}\n",
            "{'loss': 3.4642, 'grad_norm': 3.12801194190979, 'learning_rate': 0.00010454545454545455, 'epoch': 0.5}\n",
            "{'loss': 3.6194, 'grad_norm': 2.089003801345825, 'learning_rate': 0.00010202020202020202, 'epoch': 0.52}\n",
            "{'loss': 3.5536, 'grad_norm': 2.3491194248199463, 'learning_rate': 9.94949494949495e-05, 'epoch': 0.53}\n",
            "{'loss': 3.3556, 'grad_norm': 2.282104969024658, 'learning_rate': 9.696969696969698e-05, 'epoch': 0.54}\n",
            " 54% 450/834 [05:14<04:26,  1.44it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.58it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.30it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.131276607513428, 'eval_accuracy': 0.32047244094488186, 'eval_runtime': 0.788, 'eval_samples_per_second': 12.691, 'eval_steps_per_second': 5.076, 'epoch': 0.54}\n",
            " 54% 450/834 [05:15<04:26,  1.44it/s]\n",
            "100% 4/4 [00:00<00:00,  6.87it/s]\u001b[A\n",
            "{'loss': 3.3535, 'grad_norm': 2.2808494567871094, 'learning_rate': 9.444444444444444e-05, 'epoch': 0.55}\n",
            "{'loss': 3.5333, 'grad_norm': 2.9912636280059814, 'learning_rate': 9.191919191919192e-05, 'epoch': 0.56}\n",
            "{'loss': 3.2501, 'grad_norm': 2.3337061405181885, 'learning_rate': 8.93939393939394e-05, 'epoch': 0.58}\n",
            "{'loss': 3.4502, 'grad_norm': 2.401749610900879, 'learning_rate': 8.686868686868688e-05, 'epoch': 0.59}\n",
            "{'loss': 3.4295, 'grad_norm': 2.4373841285705566, 'learning_rate': 8.434343434343435e-05, 'epoch': 0.6}\n",
            " 60% 500/834 [05:50<03:51,  1.44it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.04it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.36it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.075664043426514, 'eval_accuracy': 0.32598425196850395, 'eval_runtime': 0.7772, 'eval_samples_per_second': 12.866, 'eval_steps_per_second': 5.146, 'epoch': 0.6}\n",
            " 60% 500/834 [05:51<03:51,  1.44it/s]\n",
            "100% 4/4 [00:00<00:00,  6.98it/s]\u001b[A\n",
            "{'loss': 3.4323, 'grad_norm': 2.4484729766845703, 'learning_rate': 8.181818181818183e-05, 'epoch': 0.61}\n",
            "{'loss': 3.3604, 'grad_norm': 2.666034698486328, 'learning_rate': 7.92929292929293e-05, 'epoch': 0.62}\n",
            "{'loss': 3.2343, 'grad_norm': 2.6482224464416504, 'learning_rate': 7.676767676767676e-05, 'epoch': 0.64}\n",
            "{'loss': 3.1148, 'grad_norm': 2.2564210891723633, 'learning_rate': 7.424242424242424e-05, 'epoch': 0.65}\n",
            "{'loss': 3.4911, 'grad_norm': 2.7995285987854004, 'learning_rate': 7.171717171717171e-05, 'epoch': 0.66}\n",
            " 66% 550/834 [06:26<03:17,  1.44it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.78it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.32it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.046118259429932, 'eval_accuracy': 0.3291338582677165, 'eval_runtime': 0.7822, 'eval_samples_per_second': 12.784, 'eval_steps_per_second': 5.114, 'epoch': 0.66}\n",
            " 66% 550/834 [06:27<03:17,  1.44it/s]\n",
            "100% 4/4 [00:00<00:00,  6.95it/s]\u001b[A\n",
            "{'loss': 3.4465, 'grad_norm': 2.508671522140503, 'learning_rate': 6.91919191919192e-05, 'epoch': 0.67}\n",
            "{'loss': 3.4449, 'grad_norm': 2.868577480316162, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.68}\n",
            "{'loss': 3.3653, 'grad_norm': 2.9379711151123047, 'learning_rate': 6.414141414141415e-05, 'epoch': 0.7}\n",
            "{'loss': 3.4942, 'grad_norm': 2.8210911750793457, 'learning_rate': 6.161616161616162e-05, 'epoch': 0.71}\n",
            "{'loss': 3.4685, 'grad_norm': 2.3960371017456055, 'learning_rate': 5.90909090909091e-05, 'epoch': 0.72}\n",
            " 72% 600/834 [07:02<02:43,  1.43it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.01it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.35it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.033333778381348, 'eval_accuracy': 0.3299212598425197, 'eval_runtime': 0.784, 'eval_samples_per_second': 12.754, 'eval_steps_per_second': 5.102, 'epoch': 0.72}\n",
            " 72% 600/834 [07:03<02:43,  1.43it/s]\n",
            "100% 4/4 [00:00<00:00,  6.96it/s]\u001b[A\n",
            "{'loss': 3.4156, 'grad_norm': 2.372702121734619, 'learning_rate': 5.6565656565656563e-05, 'epoch': 0.73}\n",
            "{'loss': 3.4255, 'grad_norm': 2.3764824867248535, 'learning_rate': 5.4040404040404044e-05, 'epoch': 0.74}\n",
            "{'loss': 3.3196, 'grad_norm': 2.3454620838165283, 'learning_rate': 5.151515151515152e-05, 'epoch': 0.76}\n",
            "{'loss': 3.4547, 'grad_norm': 2.131715774536133, 'learning_rate': 4.898989898989899e-05, 'epoch': 0.77}\n",
            "{'loss': 3.3803, 'grad_norm': 2.452407121658325, 'learning_rate': 4.6464646464646464e-05, 'epoch': 0.78}\n",
            " 78% 650/834 [07:38<02:07,  1.44it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.03it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.37it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.010910511016846, 'eval_accuracy': 0.33070866141732286, 'eval_runtime': 0.8017, 'eval_samples_per_second': 12.474, 'eval_steps_per_second': 4.99, 'epoch': 0.78}\n",
            " 78% 650/834 [07:39<02:07,  1.44it/s]\n",
            "100% 4/4 [00:00<00:00,  6.62it/s]\u001b[A\n",
            "{'loss': 3.2323, 'grad_norm': 2.353172540664673, 'learning_rate': 4.3939393939393944e-05, 'epoch': 0.79}\n",
            "{'loss': 3.4867, 'grad_norm': 2.579019546508789, 'learning_rate': 4.141414141414142e-05, 'epoch': 0.8}\n",
            "{'loss': 3.2934, 'grad_norm': 2.5472006797790527, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.82}\n",
            "{'loss': 3.4087, 'grad_norm': 2.411983013153076, 'learning_rate': 3.6363636363636364e-05, 'epoch': 0.83}\n",
            "{'loss': 3.4295, 'grad_norm': 2.521305561065674, 'learning_rate': 3.3838383838383844e-05, 'epoch': 0.84}\n",
            " 84% 700/834 [08:14<01:33,  1.44it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.02it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.40it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.0211501121521, 'eval_accuracy': 0.33937007874015745, 'eval_runtime': 0.7852, 'eval_samples_per_second': 12.736, 'eval_steps_per_second': 5.094, 'epoch': 0.84}\n",
            " 84% 700/834 [08:15<01:33,  1.44it/s]\n",
            "100% 4/4 [00:00<00:00,  6.88it/s]\u001b[A\n",
            "{'loss': 3.3944, 'grad_norm': 2.7384440898895264, 'learning_rate': 3.131313131313132e-05, 'epoch': 0.85}\n",
            "{'loss': 3.3347, 'grad_norm': 2.7477524280548096, 'learning_rate': 2.878787878787879e-05, 'epoch': 0.86}\n",
            "{'loss': 3.2707, 'grad_norm': 2.5203871726989746, 'learning_rate': 2.6262626262626268e-05, 'epoch': 0.88}\n",
            "{'loss': 3.3815, 'grad_norm': 2.558673620223999, 'learning_rate': 2.3737373737373738e-05, 'epoch': 0.89}\n",
            "{'loss': 3.3631, 'grad_norm': 2.3831448554992676, 'learning_rate': 2.1212121212121215e-05, 'epoch': 0.9}\n",
            " 90% 750/834 [08:50<00:58,  1.44it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.04it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.38it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.008097171783447, 'eval_accuracy': 0.33700787401574805, 'eval_runtime': 0.7809, 'eval_samples_per_second': 12.805, 'eval_steps_per_second': 5.122, 'epoch': 0.9}\n",
            " 90% 750/834 [08:50<00:58,  1.44it/s]\n",
            "100% 4/4 [00:00<00:00,  6.94it/s]\u001b[A\n",
            "{'loss': 3.3525, 'grad_norm': 2.6859259605407715, 'learning_rate': 1.8686868686868688e-05, 'epoch': 0.91}\n",
            "{'loss': 3.4639, 'grad_norm': 2.8721001148223877, 'learning_rate': 1.6161616161616165e-05, 'epoch': 0.92}\n",
            "{'loss': 3.3647, 'grad_norm': 2.8202948570251465, 'learning_rate': 1.3636363636363637e-05, 'epoch': 0.94}\n",
            "{'loss': 3.3026, 'grad_norm': 2.4896481037139893, 'learning_rate': 1.1111111111111112e-05, 'epoch': 0.95}\n",
            "{'loss': 3.3141, 'grad_norm': 2.5668506622314453, 'learning_rate': 8.585858585858587e-06, 'epoch': 0.96}\n",
            " 96% 800/834 [09:26<00:23,  1.44it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.03it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.40it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.0051445960998535, 'eval_accuracy': 0.33385826771653543, 'eval_runtime': 0.786, 'eval_samples_per_second': 12.723, 'eval_steps_per_second': 5.089, 'epoch': 0.96}\n",
            " 96% 800/834 [09:27<00:23,  1.44it/s]\n",
            "100% 4/4 [00:00<00:00,  6.98it/s]\u001b[A\n",
            "{'loss': 3.3617, 'grad_norm': 2.5886237621307373, 'learning_rate': 6.060606060606061e-06, 'epoch': 0.97}\n",
            "{'loss': 3.2715, 'grad_norm': 2.5597589015960693, 'learning_rate': 3.5353535353535352e-06, 'epoch': 0.98}\n",
            "{'loss': 3.5105, 'grad_norm': 3.1527531147003174, 'learning_rate': 1.0101010101010103e-06, 'epoch': 1.0}\n",
            "{'train_runtime': 592.1052, 'train_samples_per_second': 4.226, 'train_steps_per_second': 1.409, 'train_loss': 3.4546248172398664, 'epoch': 1.0}\n",
            "100% 834/834 [09:52<00:00,  1.41it/s]\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  total_flos               =   648356GF\n",
            "  train_loss               =     3.4546\n",
            "  train_runtime            = 0:09:52.10\n",
            "  train_samples            =       2502\n",
            "  train_samples_per_second =      4.226\n",
            "  train_steps_per_second   =      1.409\n",
            "\u001b[32m2025-04-25 17:09:03.581\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m733\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 592.1052, 'train_samples_per_second': 4.226, 'train_steps_per_second': 1.409, 'total_flos': 696167143243776.0, 'train_loss': 3.4546248172398664, 'epoch': 1.0, 'train_samples': 2502}\u001b[0m\n",
            "\u001b[32m2025-04-25 17:09:03.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m734\u001b[0m - \u001b[1mSaving model checkpoint to outputs-pt-v1\u001b[0m\n",
            "\u001b[32m2025-04-25 17:09:04.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m742\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "100% 4/4 [00:00<00:00,  7.08it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        1.0\n",
            "  eval_accuracy           =     0.3378\n",
            "  eval_loss               =     4.0028\n",
            "  eval_runtime            = 0:00:00.92\n",
            "  eval_samples            =         10\n",
            "  eval_samples_per_second =     10.836\n",
            "  eval_steps_per_second   =      4.334\n",
            "  perplexity              =    54.7489\n",
            "\u001b[32m2025-04-25 17:09:05.896\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m755\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': 4.002756595611572, 'eval_accuracy': 0.33779527559055117, 'eval_runtime': 0.9229, 'eval_samples_per_second': 10.836, 'eval_steps_per_second': 4.334, 'epoch': 1.0, 'eval_samples': 10, 'perplexity': 54.74886268540698}\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python ./MedicalGPT/pretraining.py \\\n",
        "    --model_name_or_path Qwen/Qwen2.5-0.5B \\\n",
        "    --train_file_dir ./MedicalGPT/data/pretrain \\\n",
        "    --validation_file_dir ./MedicalGPT/data/pretrain \\\n",
        "    --per_device_train_batch_size 3 \\\n",
        "    --per_device_eval_batch_size 3 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_peft True \\\n",
        "    --seed 42 \\\n",
        "    --bf16 \\\n",
        "    --max_train_samples 20000 \\\n",
        "    --max_eval_samples 10 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --warmup_ratio 0.05 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --eval_steps 50 \\\n",
        "    --evaluation_strategy steps \\\n",
        "    --save_steps 50 \\\n",
        "    --save_strategy steps \\\n",
        "    --save_total_limit 3 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --preprocessing_num_workers 1 \\\n",
        "    --block_size 128 \\\n",
        "    --group_by_length True \\\n",
        "    --output_dir outputs-pt-v1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --ddp_timeout 30000 \\\n",
        "    --logging_first_step True \\\n",
        "    --target_modules all \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --device_map auto \\\n",
        "    --report_to tensorboard \\\n",
        "    --ddp_find_unused_parameters False \\\n",
        "    --gradient_checkpointing True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ht-lZ1djzYeo",
        "outputId": "4b490390-155c-454d-977a-449c698801a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'outputs-pt-v1': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "%ls -lh outputs-pt-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TePpboqXzYeo"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "JqsODCvCzYeo"
      },
      "source": [
        "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "w2QBktw0zYeo",
        "outputId": "ffd11678-b959-4f4b-9821-2241e0d86b60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-25 16:41:39.119458: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745599299.139778    2537 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745599299.145722    2537 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-25 16:41:39.165571: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Namespace(base_model='Qwen/Qwen2.5-0.5B', tokenizer_path=None, lora_model='outputs-pt-v1', resize_emb=False, output_dir='merged-pt/', hf_hub_model_id='', hf_hub_token=None)\n",
            "Base model: Qwen/Qwen2.5-0.5B\n",
            "LoRA model: outputs-pt-v1\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/models.py\", line 1024, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/outputs-pt-v1/resolve/main/adapter_config.json\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/peft/config.py\", line 195, in from_pretrained\n",
            "    config_file = hf_hub_download(\n",
            "                  ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 961, in hf_hub_download\n",
            "    return _hf_hub_download_to_cache_dir(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 1068, in _hf_hub_download_to_cache_dir\n",
            "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 1596, in _raise_on_head_call_error\n",
            "    raise head_call_error\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 1484, in _get_metadata_or_catch_error\n",
            "    metadata = get_hf_file_metadata(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 1401, in get_hf_file_metadata\n",
            "    r = _request_wrapper(\n",
            "        ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 285, in _request_wrapper\n",
            "    response = _request_wrapper(\n",
            "               ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 309, in _request_wrapper\n",
            "    hf_raise_for_status(response)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\", line 459, in hf_raise_for_status\n",
            "    raise _format(RepositoryNotFoundError, message, response) from e\n",
            "huggingface_hub.errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-680bbb46-538247141b22832e79f627a4;228ec538-0795-4468-86c0-8a2d8882062b)\n",
            "\n",
            "Repository Not Found for url: https://huggingface.co/outputs-pt-v1/resolve/main/adapter_config.json.\n",
            "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
            "If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\n",
            "Invalid username or password.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/MedicalGPT/merge_peft_adapter.py\", line 104, in <module>\n",
            "    main()\n",
            "  File \"/content/MedicalGPT/merge_peft_adapter.py\", line 45, in main\n",
            "    peft_config = PeftConfig.from_pretrained(lora_model_path)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/peft/config.py\", line 199, in from_pretrained\n",
            "    raise ValueError(f\"Can't find '{CONFIG_NAME}' at '{pretrained_model_name_or_path}'\") from exc\n",
            "ValueError: Can't find 'adapter_config.json' at 'outputs-pt-v1'\n"
          ]
        }
      ],
      "source": [
        "!python merge_peft_adapter.py \\\n",
        "    --base_model Qwen/Qwen2.5-0.5B --lora_model outputs-pt-v1 --output_dir merged-pt/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yK2buV_LzYeo",
        "outputId": "bac488cb-96ed-488f-9da3-27ff4cb18194",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'merged-pt/': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "%ls -lh merged-pt/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FAknjGCvzYep",
        "outputId": "31501d81-8462-4127-95f2-c240a55930d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat: merged-pt/config.json: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "%cat merged-pt/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gdLLb2yzYep"
      },
      "source": [
        "Stage1 增量预训练完成。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-15T13:56:17.081153Z",
          "start_time": "2023-06-15T13:56:17.032821Z"
        },
        "id": "iaUvLwZCzYep"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "xjiFzrslzYep"
      },
      "source": [
        "# Stage 2: Supervised FineTuning\n",
        "\n",
        "第二阶段：SFT(Supervised Fine-tuning)有监督微调，构造指令微调数据集，在预训练模型基础上做指令精调，以对齐指令意图，并注入领域知识\n",
        "\n",
        "| Stage 2: Supervised Fine-tuning | [supervised_finetuning.py](https://github.com/shibing624/MedicalGPT/blob/main/supervised_finetuning.py) | [run_sft.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_sft.sh)  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "zZ10uCZCzYep"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是Qwen/Qwen2.5-0.5B 或者 Stage1得到的预训练模型\n",
        "2. 数据集：SFT阶段使用的是使用的是Belle的1千条抽样数据，位于`data/finetune`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "79JXL2jpzYep"
      },
      "source": [
        "## Stage2 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载模型和tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-15T13:58:38.966506Z",
          "start_time": "2023-06-15T13:58:38.778132Z"
        },
        "id": "uKBayxLxzYep",
        "outputId": "5bd08ba9-f0d2-4e6d-a014-7d4c35fe4ea6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "medical_sft_1K_format.jsonl  sharegpt_zh_1K_format.jsonl\n"
          ]
        }
      ],
      "source": [
        "%ls ./data/finetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "j8Dv8hQFzYep",
        "outputId": "b9aaf006-917b-4b03-d959-7c65b14eaa52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/MedicalGPT/supervised_finetuning.py\", line 31, in <module>\n",
            "    from datasets import load_dataset\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/__init__.py\", line 17, in <module>\n",
            "    from .arrow_dataset import Dataset\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\", line 69, in <module>\n",
            "    from multiprocess import Pool\n",
            "ModuleNotFoundError: No module named 'multiprocess'\n"
          ]
        }
      ],
      "source": [
        "!python supervised_finetuning.py \\\n",
        "    --model_name_or_path merged-pt \\\n",
        "    --train_file_dir ./data/finetune \\\n",
        "    --validation_file_dir ./data/finetune \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --per_device_eval_batch_size 4 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_peft True \\\n",
        "    --bf16 \\\n",
        "    --max_train_samples 1000 \\\n",
        "    --max_eval_samples 10 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --warmup_ratio 0.05 \\\n",
        "    --weight_decay 0.05 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --eval_steps 50 \\\n",
        "    --evaluation_strategy steps \\\n",
        "    --save_steps 500 \\\n",
        "    --save_strategy steps \\\n",
        "    --save_total_limit 3 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --preprocessing_num_workers 1 \\\n",
        "    --output_dir outputs-sft-v1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --ddp_timeout 30000 \\\n",
        "    --logging_first_step True \\\n",
        "    --target_modules all \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --device_map auto \\\n",
        "    --report_to tensorboard \\\n",
        "    --ddp_find_unused_parameters False \\\n",
        "    --gradient_checkpointing True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MZPL5H0OzYep",
        "outputId": "009af8d8-923d-403d-9774-09baea50db33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'outputs-sft-v1': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "%ls -lh outputs-sft-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "kuTdzT3tzYep"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "UCcQdVRezYeq"
      },
      "source": [
        "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BzDN7huLzYeq",
        "outputId": "bade85f6-0e2d-444b-90a3-d37165cbf07d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-25 16:42:25.678916: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745599345.699118    2766 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745599345.705423    2766 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-25 16:42:25.727048: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Namespace(base_model='merged-pt', tokenizer_path=None, lora_model='outputs-sft-v1', resize_emb=False, output_dir='./merged-sft', hf_hub_model_id='', hf_hub_token=None)\n",
            "Base model: merged-pt\n",
            "LoRA model: outputs-sft-v1\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/models.py\", line 1024, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/outputs-sft-v1/resolve/main/adapter_config.json\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/peft/config.py\", line 195, in from_pretrained\n",
            "    config_file = hf_hub_download(\n",
            "                  ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 961, in hf_hub_download\n",
            "    return _hf_hub_download_to_cache_dir(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 1068, in _hf_hub_download_to_cache_dir\n",
            "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 1596, in _raise_on_head_call_error\n",
            "    raise head_call_error\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 1484, in _get_metadata_or_catch_error\n",
            "    metadata = get_hf_file_metadata(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 1401, in get_hf_file_metadata\n",
            "    r = _request_wrapper(\n",
            "        ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 285, in _request_wrapper\n",
            "    response = _request_wrapper(\n",
            "               ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 309, in _request_wrapper\n",
            "    hf_raise_for_status(response)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\", line 459, in hf_raise_for_status\n",
            "    raise _format(RepositoryNotFoundError, message, response) from e\n",
            "huggingface_hub.errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-680bbb74-509c076e49bb6fed3e6520ef;e49499bb-4cc3-446d-ad4e-468b40420af8)\n",
            "\n",
            "Repository Not Found for url: https://huggingface.co/outputs-sft-v1/resolve/main/adapter_config.json.\n",
            "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
            "If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\n",
            "Invalid username or password.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/MedicalGPT/merge_peft_adapter.py\", line 104, in <module>\n",
            "    main()\n",
            "  File \"/content/MedicalGPT/merge_peft_adapter.py\", line 45, in main\n",
            "    peft_config = PeftConfig.from_pretrained(lora_model_path)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/peft/config.py\", line 199, in from_pretrained\n",
            "    raise ValueError(f\"Can't find '{CONFIG_NAME}' at '{pretrained_model_name_or_path}'\") from exc\n",
            "ValueError: Can't find 'adapter_config.json' at 'outputs-sft-v1'\n"
          ]
        }
      ],
      "source": [
        "!python merge_peft_adapter.py \\\n",
        "    --base_model merged-pt --lora_model outputs-sft-v1 --output_dir ./merged-sft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3I8QT1y5zYeq",
        "outputId": "1c8dfe0f-ab99-4495-fe83-8d47098def17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'merged-sft/': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "%ls -lh merged-sft/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0DN84rpezYeq",
        "outputId": "9bc10325-0041-4929-929f-294d2b6ccd26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat: merged-sft/config.json: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "%cat merged-sft/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "C9FOgbhbzYeq"
      },
      "source": [
        "Stage2 SFT训练完成。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-15T14:07:40.752635Z",
          "start_time": "2023-06-15T14:07:40.731186Z"
        },
        "id": "9qhKDBV2zYeq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "EW75IWZXzYeq"
      },
      "source": [
        "# Stage 3: DPO(Direct Preference Optimization)\n",
        "\n",
        "第三阶段：DPO(Direct Preference Optimization)直接偏好优化，DPO通过直接优化语言模型来实现对其行为的精确控制，而无需使用复杂的强化学习，也可以有效学习到人类偏好，DPO相较于RLHF更容易实现且易于训练，效果更好\n",
        "\n",
        "| Stage 3: Direct Preference Optimization        |  [dpo_training.py](https://github.com/shibing624/MedicalGPT/blob/main/dpo_training.py) | [run_dpo.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_dpo.sh)    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "HAoYv4mhzYeq"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是`Qwen/Qwen2.5-0.5B` 或者 Stage2得到的SFT模型\n",
        "2. 数据集：DPO阶段使用的是医疗reward数据，抽样了500条，位于`data/reward`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5qLQZgfszYeq"
      },
      "source": [
        "## Stage3 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载模型和tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "o6pFBULszYeq",
        "outputId": "d8c7f337-9b0c-4b2b-9fb9-b2b9e6a956a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dpo_zh_500.jsonl\n"
          ]
        }
      ],
      "source": [
        "%ls ./data/reward/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Z62koXoAzYeq",
        "outputId": "031d2dd8-d50a-4542-fbe6-15892ab3e6ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/MedicalGPT/dpo_training.py\", line 13, in <module>\n",
            "    from datasets import load_dataset\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/__init__.py\", line 17, in <module>\n",
            "    from .arrow_dataset import Dataset\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\", line 69, in <module>\n",
            "    from multiprocess import Pool\n",
            "ModuleNotFoundError: No module named 'multiprocess'\n"
          ]
        }
      ],
      "source": [
        "!python dpo_training.py \\\n",
        "    --model_name_or_path ./merged-sft \\\n",
        "    --template_name qwen \\\n",
        "    --train_file_dir ./data/reward \\\n",
        "    --validation_file_dir ./data/reward \\\n",
        "    --per_device_train_batch_size 3 \\\n",
        "    --per_device_eval_batch_size 1 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_peft True \\\n",
        "    --max_train_samples 1000 \\\n",
        "    --max_eval_samples 500 \\\n",
        "    --max_steps 100 \\\n",
        "    --eval_steps 10 \\\n",
        "    --save_steps 50 \\\n",
        "    --max_source_length 256 \\\n",
        "    --max_target_length 256 \\\n",
        "    --output_dir outputs-dpo-v1 \\\n",
        "    --target_modules all \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --bf16 True \\\n",
        "    --fp16 False \\\n",
        "    --device_map auto \\\n",
        "    --report_to tensorboard \\\n",
        "    --remove_unused_columns False \\\n",
        "    --gradient_checkpointing True \\\n",
        "    --cache_dir ./cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwYDy1nszYeq"
      },
      "outputs": [],
      "source": [
        "%ls -lh outputs-dpo-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "7rlQrXZLzYeq"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "OOZhdQv4zYer"
      },
      "source": [
        "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3dx5l6pzYer"
      },
      "outputs": [],
      "source": [
        "!python merge_peft_adapter.py \\\n",
        "    --base_model merged-sft --lora_model outputs-dpo-v1 --output_dir merged-dpo/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljfD_jMWzYeu"
      },
      "outputs": [],
      "source": [
        "%ls -lh merged-dpo/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QZg9U5FzYeu"
      },
      "outputs": [],
      "source": [
        "%cat merged-dpo/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "T46PceGgzYev"
      },
      "source": [
        "Stage3 偏好建模第一次训练完成。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "CDSEcOzKzYev"
      },
      "source": [
        "**至此一个完整的训练流程演示完成。**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-26T12:34:29.658428Z",
          "start_time": "2023-06-26T12:34:29.620609Z"
        },
        "id": "qSYzcFCdzYev"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "oahv_vZyzYev"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-26T12:35:00.864463Z",
          "start_time": "2023-06-26T12:34:47.802087Z"
        },
        "id": "5s8sGUpFzYev"
      },
      "outputs": [],
      "source": [
        "!python inference.py --base_model merged-dpo\n",
        "# 或在shell中运行\n",
        "# python inference.py --base_model merged-dpo --interactive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "PHbP8-KuzYev"
      },
      "source": [
        "Input:介绍下南京\n",
        "Response:  南京市位于江苏省西南部，是全国首批历史文化名城、国家中心城市和自由贸易试验区。\n",
        "\n",
        "完。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n5QJQXKzYev"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "f34eed0bebedfc4b6ee51ced43d2c030fe3b92f13c149d072205ca200a67b1ec"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}